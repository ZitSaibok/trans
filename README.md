模型的微调过程是将模型的全部参数进行更新，由于参数量大，在小样本学习时容易过拟合，导致测试性能较差。这种微调过程可以视为把预训练的权重参数W更新为W+ΔW，由于ΔW和W是完全相同的张量，因此参数量大。基于张量分解的参数高效微调方法可以理解为，将ΔW分解（也可以说是重参数化）为若干个参数量小的因子的张量乘积，例如ΔW=A*B*C，则微调过程变为优化W+A*B*C。由于A、B、C的参数量小，从而减少了优化的参数空间，避免过拟合。

Transformer层输入的是长度可变的token序列，其计算量与token序列的长度至少成正比（部分算子为二次方），因此可以通过融合相似的token以及裁剪信息量低的token来减少输入长度，从而减少计算量。
